{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3e14d3b",
   "metadata": {},
   "source": [
    "# Lab 2\n",
    "\n",
    "In the previous lab, you have seen how numpy works and got to experiment with it. In the next part of this tutorial, you will implement a machine learning algorithm on a simple classification problem. The goal is for you to familiarise yourselves with the different tools and libraries available in python including:\n",
    "\n",
    "**Scikit-Learn** - a very popular machine learning library, containing implementations of various classification, regression and clustering algorithms such as SVM, Random Forests, gradient boosting, k-means. It also contains a few datasets which are imported together with the package, one of wich will be used in this tutorial.\n",
    "\n",
    "Documentation: https://scikit-learn.org/stable/modules/classes.html\n",
    "\n",
    "**Matplotlib** - a python library used to generate visual representations (plots) of data and different mathematical procedures. It has an active community of developers and users, thus making it easy to find tutorials and help with errors. In this lab, you will get to experiment with its basic functionalities, but be mindful that matplotlib can be used to generate complex diagrams.\n",
    "\n",
    "Documentation and extra Tutorials: https://matplotlib.org/stable/tutorials/index\n",
    "\n",
    "**Pandas** - it is another python library used for data manipulation. The main diference from numpy is that Pandas is mainly used for data analysis and preprocessing, and less for mathematical operations. The main data structure implemented within pandas are its *data frames*, which you will see working in this lab. \n",
    "Hint: it uses the dictionary structure mentioned in Lab1 Homework bonus\n",
    "\n",
    "Documentation: https://pandas.pydata.org/docs/\n",
    "\n",
    "Radu-Daniel Voit, Last update 31/01/2022\n",
    "\n",
    "Copyright University of Southampton, 2022. Permission is granted for copies to be made for personal use by University of Southampton students. This content should not be shared on published outside the University of Southampton.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa0b082a",
   "metadata": {},
   "source": [
    "## Problem definition\n",
    "\n",
    "We will import the **Breast Cancer Wisconsin** dataset from the Scikit-Learn library. It contains 569 instances, each having 32 attributes. Each instance represents a cancer scan, and the goal of this task is to clasify whether an instance represents a benings or malignan cancerous scan.\n",
    "\n",
    "**Note:** Although this dataset is not specific for a NLP problem, you will encounter the same structural forms (data with multiple instances and features, represented as a matrix). However, in NLP, the matrix can represent a *TF-IDF*, or *PPMI*, as you will see during the module.\n",
    "\n",
    "First, we import the usefull libraries and the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9356be8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this code\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "Data = load_breast_cancer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e2b82b",
   "metadata": {},
   "source": [
    "Please write the line of code that prints the type of the variable *Data*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b557267f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "312ac234",
   "metadata": {},
   "source": [
    "As you can see, its type is defined within the sklearn library. As mentioned in the previous lab, it is important to pay attention to this as it can lead errors. \n",
    "\n",
    "This type in particular works similar to the default python dictionary. To see the keys, run the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe4de75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this code \n",
    "\n",
    "Data.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "068b1e27",
   "metadata": {},
   "source": [
    "In order to access the values stored by each key of a dictionary, one can simply use the *your_dictionary[key]* command. However, the type defined within sklearn allows for an easier calling method\n",
    "\n",
    "Run the following code and complete it in order to print the number of datapoints, features, in order to confirm the documentation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce20307",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run and complete the code:\n",
    "\n",
    "print(\"The data contains\", len(Data.data), \"points\")\n",
    "print()\n",
    "#print(\"There are\", - YOUR CODE HERE -, \"features\")\n",
    "print()\n",
    "#print(\"There are\", - YOUR CODE HERE -, \"different target labels\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f63bb5",
   "metadata": {},
   "source": [
    "If you want to also check the name of the categories and whether or not every instance is labelled, please write the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e6d61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b272d6d6",
   "metadata": {},
   "source": [
    "Now, lets create a pandas dataframe instance and fill it with our data. But first, please check the type and shape of *Data.data*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61ee272",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94858b89",
   "metadata": {},
   "source": [
    "The data is stored as a numpy array. Let's load it into a pandas dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b734564a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this code\n",
    "\n",
    "# Create pandas dataframe\n",
    "# pass the feature names as the labels for columns\n",
    "# check documentation for more parameters of DataFrame\n",
    "\n",
    "df = pd.DataFrame(Data.data, columns= Data.feature_names)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c39a09",
   "metadata": {},
   "source": [
    "**Note:** Pandas can read data automatically from files with extensions like .csv or .json. Consult the documentation for more information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a24ee3",
   "metadata": {},
   "source": [
    "## Data analysis\n",
    "\n",
    "Using the newly created dataframe, we can now test some of the methods associated with it. For instance, if you want to see the first n rows, use the *.head(n)* method, where the default value for n is 5. Run the code below for an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f4b277",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run this code\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c349dac0",
   "metadata": {},
   "source": [
    "Now, using the documentation, please write the code that generates statistics about the dataset and provides a concise summary of *df*. \n",
    "\n",
    "**Note:** You can see all methods in the DataFrame class here: https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24fa24be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write code here\n",
    "\n",
    "# Hint: look for describe() and info() methods\n",
    "# Hint: it is not necessary to pass parameters to the methods\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef345059",
   "metadata": {},
   "source": [
    "Methods can be applied on specific columns in the data frame as well. Run the code below. What can be done with this information? What type of data (e.g. numerical, categorical) can benefit from this kind of functions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d6e559d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this code\n",
    "df[\"mean area\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f0e985b",
   "metadata": {},
   "source": [
    "Some of the raw datasets that you could work with don't separate the targets from the rest of the data. Or maybe you prefer to have all data in a single dataframe. Complete and run the code below to concatenate the labels column to *df*. Use the documentation for guidance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5cf28e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete and run the code:\n",
    "\n",
    "target_df = pd.DataFrame(Data.target, columns=[\"Target\"])\n",
    "print(target_df)\n",
    "print()\n",
    "\n",
    "df = pd.concat(#- your code here-)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820819a3",
   "metadata": {},
   "source": [
    "Now, use the previous analysis methods to check whther the targets were corectly added to the frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a823b8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "863717bc",
   "metadata": {},
   "source": [
    "Use the corect method to print how many datapoints are associated with each label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23dc1a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b1cb0e",
   "metadata": {},
   "source": [
    "## Visual representations\n",
    "\n",
    "Before starting to implement any machine learning model, you will need to thorougly analyse the data for information on its sparsity, distribution, etc. However, in the ML community, it is important to be able to communicate information clearly and concisely. Visual representations are generally preffered as a way of communicating statistical information. \n",
    "\n",
    "Run the code below. How can you interpret the result?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc1d8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the following code\n",
    "\n",
    "# This is called a magic function in IPython that helps with the page organisation\n",
    "%matplotlib inline \n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "df.hist(bins=30, figsize=(20,15)) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99438ef5",
   "metadata": {},
   "source": [
    "Modify and play with parameters if you want, see how it affects the output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "215bfa5e",
   "metadata": {},
   "source": [
    "Now, lets generate a more complex graph. When analysing the data, you might want to see how labels are distributed among different features. \n",
    "\n",
    "Lets take into consideration two of our features, *mean radius* and *mean smoothness*, for example.  Complete the code below to generate a plot. Play with the different variables such as colors and figsize, and check the documentation for the other methods used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9b0371",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Complete and run the code\n",
    "\n",
    "import matplotlib\n",
    "\n",
    "df = pd.concat([df,target_df], axis = 1)\n",
    "\n",
    "df[\"Target\"].value_counts()\n",
    "target = df[\"Target\"]\n",
    "\n",
    "colors = ['red', 'blue']\n",
    "\n",
    "fig = plt.figure(figsize=(10,10))\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d361f36",
   "metadata": {},
   "source": [
    "What do you think that this plot is missing in order to be considered a good plot for communicating the importance of the selected features?\n",
    "\n",
    "**Note:** The best practice when plotting is to always add labels for your axis and a legend for your colors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f43a37f8",
   "metadata": {},
   "source": [
    "Look again at the following code (similar to the previous). Using the documentation, please complete the code in order to add labels for your axis. Also, please add a color legend in order to understand which label is represented as blue and which is red. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a926d790",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete and run the code\n",
    "\n",
    "import matplotlib\n",
    "\n",
    "radius = df['mean radius'] \n",
    "smooth = df['mean smoothness'] \n",
    "target = df['Target'] \n",
    "colors = ['red', 'blue']\n",
    "\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "scatter = plt.scatter(radius, smooth, c=target, cmap=matplotlib.colors.ListedColormap(colors))\n",
    "\n",
    "# Hint: check what can be used from matplotlib.pyplot (e.g. legend method) and use the documentation  \n",
    "# Hint: use parameters such as fontsize in order to make your labels and legend more readable\n",
    "\n",
    "# Your code here:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0105815a",
   "metadata": {},
   "source": [
    "**Note** There are more than one way to create and manipulate plots with matplotlib. For example, one can use matplotlib.pyplot.subplots in order to manipulate multiple subplots in the same figure. Check the documentation for more \n",
    "https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.subplots.html\n",
    "\n",
    "**Note** Matplotlib support pages usually come with plot examples, and their respective code. You can check some of them to familiarise yourself with more designs "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef714e73",
   "metadata": {},
   "source": [
    "## Data partitioning and Model training\n",
    "\n",
    "\n",
    "It is important to mention that in most real-wrold cases, and especially in NLP, the data would need to be preprocessed and \"cleaned\" of errors, missing values, capings etc.. You will get to learn more about this in the context of textual data later in the module.\n",
    "\n",
    "For now, you will only focus on how to partition the data into training and testing sets using the tools you have just learned."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21cf9c9d",
   "metadata": {},
   "source": [
    "First, you need to split the dataframe into input *X* and target *Y*. The following code uses the *pandas.DataFrame.iloc* method for spliting the dataframe based on indexes. With this ocasion, you can learn how slices work in python.\n",
    "\n",
    "As a simple rule of thumb, for an array l = [0,1,...,n], you can:\n",
    "\n",
    "    1. Select the first i items like l[:i] (this will select items up to index i-1)\n",
    "    2. Select a particular portion of the list l[i:j] (select items from index i to j-1)\n",
    "    3. Select all items starting from an index l[i:] (select items from index i to the end of the array)\n",
    "    4. Select items from index i to j by a step s l[i:j:s]\n",
    "    5. Select all using l[:]. Usefull when you want to slice only on particular dimensions in a multidimensional array\n",
    "    5. You can also use negative numbers:\n",
    "        l[-1] - selects last item\n",
    "        l[-i:] - last i items\n",
    "        l[:-i] - select all except last i items"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f55e70",
   "metadata": {},
   "source": [
    "Complete and run the code to separate our input data from the labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570941db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete and run the code:\n",
    "\n",
    "# Hint: iloc performs selection on both dimensions -> make sure you keep all rows and only select based on the columns\n",
    "# Hint: in this case, the labels/targets are on the last column.\n",
    "\n",
    "X = # Your code here\n",
    "Y = # Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5491fd08",
   "metadata": {},
   "source": [
    "Now, run the next code to perform the train-test split using the scikit-learn  method *train_test_split*. You can read more about it here:\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff84899",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this code:\n",
    "\n",
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "X_train, X_test, Y_train, Y_test=train_test_split(X,Y,test_size = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50492862",
   "metadata": {},
   "source": [
    "**Note:** the *train_test_split* shuffles the data by default. In order to obtain reproducable results, a random seed should be generated and passed to the random_state parameter.\n",
    "\n",
    "What does *test_size = 0.2* mean? Write the code to check the shapes of the resulted variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b325f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write code here:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abdb7291",
   "metadata": {},
   "source": [
    "The data is not split and prepared to be fed to a model. As previousely mentioned, scikit learn library has a multitude of model implementations you can choose from. Considering this classification problem, choose a suitable model (Random Forest, SVM, KNeighbours, etc.) and instantiate it in the following code block. \n",
    "\n",
    "**Note:** Choose your hyperparameters as you want, your goal is to implement a functional model for now. You will deal with the accuracy of models in NLP problems later during the module.\n",
    "\n",
    "Use the documentation as needed:https://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb475730",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete and run the code\n",
    "\n",
    "from sklearn # -Your code here - \n",
    "\n",
    "model = # -Your code here-"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "531f1542",
   "metadata": {},
   "source": [
    "Now, fit the model to our data by running the next code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba95a16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this code:\n",
    "model.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42494069",
   "metadata": {},
   "source": [
    "\n",
    "Again, using the documentation, generate predictions on the *X_test* set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee11319",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write code here\n",
    "prediction = # -Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca2793de",
   "metadata": {},
   "source": [
    "In order to verify how good the model performed, choose a metric by which to measure. You can look up the *sklearn.metrics* to see available implemetation for metrics. \n",
    "\n",
    "Run the following code to see how your model performed using the confusion matrix. Alterate the code to use any other metric. Also, depending on the model you have used, you can check it's own *.score()* method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142dc037",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this code:\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "confusion_matrix(Y_test, prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad89579",
   "metadata": {},
   "source": [
    "How do you interpret these results?\n",
    "\n",
    "**Note:** For more complex problems, one cannot only rely on a single model or a single set of hyperparameter. In order to test multiple model settings, you can use *sklearn.model_selection.GridSearchCV*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555e17b1",
   "metadata": {},
   "source": [
    "## HOMEWORK ##\n",
    "\n",
    "\n",
    "Having experimented with **scikit learn**, **matplotlib** and **pandas** , you can now have some practice with a text-based dataset. \n",
    "\n",
    "The **20newsgroups** is a popular dataset used for experimentation with tasks such as text classification and text clustering. The dataset contains posts/messages from 20 different newsgroups. Each group is focused on a different subject. \n",
    "\n",
    "**Documentation:** http://qwone.com/~jason/20Newsgroups/\n",
    "\n",
    "Run the following code to fetch the dataset using scikitlearn. The dataset is freely available from multiple sources, including the documentation, if you will want to experiment with it. There is also a challenge on Kaggle for it: https://www.kaggle.com/crawford/20-newsgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae0d28e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this code:\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "newsgroups = fetch_20newsgroups(subset = \"all\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e77386",
   "metadata": {},
   "source": [
    "Look at the content of the dataset. What can you say about the format? How many datapoints are in the set? How is the data labelled (what are the target names?). Use the documentation if needed:\n",
    "https://scikit-learn.org/0.19/datasets/twenty_newsgroups.html#newsgroups\n",
    "\n",
    "Hint: *newsgroups* has the same type as *Data*, from the beginning of the lab. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1fa9159",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a9c777",
   "metadata": {},
   "source": [
    "### Exercise 1\n",
    "\n",
    "Using any of the python data structures learned and any available documentation, plot the top 20 most occuring words in a histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107125c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb2eecb",
   "metadata": {},
   "source": [
    "### Exercise 2\n",
    "\n",
    "Calculate the term frequency-inverse document frequency (TF-IDF) of the words in the data. It is a simple way of calculating how representative a word can be for a document within a larger set of documents (corpus). Although you will find out more about it during the module, some extra reading never hurts :)\n",
    "\n",
    "Use the following documentation for guidance:https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\n",
    "\n",
    "Using *Exercise 1*, plot the top 20 words with the best tf-idf score. Is there a difference from the previous results? If yes, why? Give it a thought."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b782b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba007e8c",
   "metadata": {},
   "source": [
    "# Solutions\n",
    "\n",
    " 1. type(Data)\n",
    " \n",
    " 2. print(\"There are\", len(Data.feature_names), \"features\")\n",
    "    print(\"There are\", len(Data.target_names), \"different target labels\")\n",
    "    \n",
    " 3. df.describe()\n",
    "    df.info()\n",
    "    \n",
    " 4. df = pd.concat([df,target_df], axis = 1)\n",
    " \n",
    " 5. df[\"Target\"].value_counts()\n",
    " \n",
    " 6. radius = df[\"mean radius\"]\n",
    "    smooth = df[\"mean smoothness\"]\n",
    "    plt.scatter(radius, smooth, c=target, cmap=matplotlib.colors.ListedColormap(colors)) \n",
    " \n",
    " \n",
    " 7. plt.xlabel(\"Mean Radius\")\n",
    "    plt.ylabel(\"Mean Smoothness\")\n",
    "    plt.legend(handles= scatter.legend_elements()[0], labels = [\"malignant\", \"benign\"], fontsize=10) \n",
    " \n",
    " 8. X = df.iloc[:, :-1]\n",
    "    Y = df.iloc[:, -1] \n",
    "    \n",
    " 9. Use .shape\n",
    " \n",
    " 10. Example with Random Forest Classifier:\n",
    " \n",
    "     from sklearn.ensemble import RandomForestClassifier  \n",
    "     model = RandomForestClassifier(max_depth=10, random_state=0)\n",
    "      \n",
    " 11. prediction = model.predict(X_test)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}